{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buyer Stage Training Data Set Generation \n",
    "### Following is the high-level ER on how the data set is generated\n",
    "----\n",
    "![title](img/buyer-intent-ER.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Following are the steps to install the package on SageMaker notebook :\n",
    "\n",
    "STEP 1: run !which python in your sell and get the location of your python packages. For example, in my case it is : /home/ec2-user/anaconda3/envs/python3/bin/python\n",
    "STEP 2: run !/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade  --ignore-installed --force-reinstall https://s3-us-west-2.amazonaws.com/move-dl-common-binary-distrubution/python/move_dl_common_api-3.2.131-release.tar.gz\n",
    "STEP 3: End (hopefully you installed it successfully!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade --ignore-installed --force-reinstall https://s3-us-west-2.amazonaws.com/move-dl-common-binary-distrubution/python/move_dl_common_api-3.2.131-release.tar.gz\n",
    "from move_dl_common_api.athena_util import AthenaUtil\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import warnings\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem()\n",
    "import warnings\n",
    "import logging\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='buyer_intent.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following are the steps to install the package on SageMaker notebook :\n",
    "\n",
    "STEP 1: run !which python in your sell and get the location of your python packages. For example, in my case it is : /home/ec2-user/anaconda3/envs/python3/bin/python\n",
    "\n",
    "STEP 2: run !/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade  --ignore-installed --force-reinstall https://s3-us-west-2.amazonaws.com/move-dl-common-binary-distrubution/python/move_dl_common_api-3.2.131-release.tar.gz\n",
    "\n",
    "STEP 3: End (hopefully you installed it successfully!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following are the main functions used to generate training data set. The SQL part will be used in building ETL pipline of the buyer intent app and training pipline. Following are the list of functions:\n",
    "#### read_config_file : read configuration file needed to make training data set. It is in the file ds_generation_config_params.json \n",
    "#### create_db : \n",
    "#### get_table_names_in_db :\n",
    "#### drop_tables_in_db :\n",
    "#### drop_table_in_db :\n",
    "#### random_sample_based_on_string_attr : \n",
    "#### copy_consp_member_id_summary_001 : Create a copy of CAP data with member_id in our development enviroment\n",
    "#### copy_consp_member_id_summary_007 :\n",
    "#### copy_consp_member_id_summary_030 :\n",
    "#### copy_consp_member_id_summary_060 :\n",
    "#### copy_consp_member_id_summary_090 :\n",
    "#### create_table_over_qualtrics_data :\n",
    "#### make_training_dataset_001_007_030_060_090 : Generate training data set contains user behaviours\n",
    "#### union_two_tables :\n",
    "#### union_all_tables :\n",
    "#### parse_time_to_mst :\n",
    "#### transform_qualtrics_survey : transform and reformat qualtric data to match its table schema\n",
    "#### label_training_data : Generate final training data set for modeling purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config_file(file_name = 'ds_generation_config_params.json'):\n",
    "    # Load Configurations of DBs\n",
    "    with open(file_name, 'r') as fp:\n",
    "        param_dict = json.load(fp)\n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(param_dict):\n",
    "    # Setup Athena Staging\n",
    "    util = AthenaUtil(s3_staging_folder = param_dict['s3_staging_path'])\n",
    "    # Create db on dev account under the name consumer-behaviour\n",
    "    ctas_query_create_consumer_behaviour_db = \"\"\"CREATE DATABASE IF NOT EXISTS {db_name};\"\"\".format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_create_consumer_behaviour_db)\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return util\n",
    "    else:\n",
    "        return util "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names_in_db(param_dict, util):     \n",
    "    try:\n",
    "        ctas_sql_table_list = \"\"\" SHOW TABLES IN {db_name};\"\"\".format(**param_dict)\n",
    "        res = util.execute_query(sql_query = ctas_sql_table_list)\n",
    "        length_lst = len(res['ResultSet']['Rows'][:])\n",
    "        lst = []\n",
    "        for i in range(0,length_lst, 1):\n",
    "            lst.append(res['ResultSet']['Rows'][:][i]['Data'][0]['VarCharValue'])\n",
    "        return lst\n",
    "    except Exception as message:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        f_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        logger.error(\"filename = %s - function = get_table_names_in_db in line_number = %s - hint = %s \" % (\n",
    "            f_name, exc_tb.tb_lineno, message.__str__()))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables_in_db(param_dict, util):\n",
    "    try:\n",
    "        table_list = get_table_names_in_db(param_dict, util)\n",
    "        for elem in table_list:\n",
    "            drop_table_in_db(param_dict, elem, util)\n",
    "        return 1\n",
    "    except Exception as message:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        f_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        logger.error(\"filename = %s - function = drop_tables_in_db - exception in line_number = %s - hint = %s \" % (\n",
    "                f_name, exc_tb.tb_lineno, message.__str__()))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table_in_db(param_dict, table_name, util):\n",
    "    try:\n",
    "        tmp_param = param_dict.copy()\n",
    "        tmp_param['table_name'] = table_name     \n",
    "        ctas_sql_drop_table = \"\"\" DROP TABLE {db_name}.{table_name};\"\"\".format(**tmp_param)\n",
    "        res = util.execute_query(sql_query = ctas_sql_drop_table)\n",
    "        del tmp_param\n",
    "        return 1\n",
    "    except Exception as message:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        f_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        logger.error(\"filename = %s - function = drop_table_in_db - exception in line_number = %s - hint = %s \" % (\n",
    "            f_name, exc_tb.tb_lineno, message.__str__()))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_based_on_string_attr(util, \n",
    "                                       db_name, \n",
    "                                       s3_path, \n",
    "                                       to_table_name, \n",
    "                                       from_table_name, \n",
    "                                       string_atr, \n",
    "                                       sample_rate = 20, \n",
    "                                       extr_loc_by_table_name = False):\n",
    "        try:\n",
    "            if extr_loc_by_table_name is True:\n",
    "                ctas_sql_random_sampling = \"\"\"CREATE TABLE {}.{}\n",
    "                WITH (\n",
    "                  external_location = '{}/tables/{}',\n",
    "                  format='PARQUET'\n",
    "                ) AS\n",
    "                SELECT * \n",
    "                FROM {}.{} \n",
    "                WHERE MOD(from_big_endian_64(xxhash64(to_utf8({}))), CAST({} AS BIGINT)) = 0;\n",
    "                \"\"\".format(db_name, \n",
    "                           to_table_name,\n",
    "                           s3_path,\n",
    "                           to_table_name,\n",
    "                           db_name, \n",
    "                           from_table_name, \n",
    "                           string_atr, \n",
    "                           str(sample_rate))\n",
    "            else:\n",
    "                ctas_sql_random_sampling = \"\"\"CREATE TABLE {}.{}\n",
    "                WITH (\n",
    "                  format='PARQUET'\n",
    "                ) AS\n",
    "                SELECT * \n",
    "                FROM {}.{} \n",
    "                WHERE MOD(from_big_endian_64(xxhash64(to_utf8({}))), CAST({} AS BIGINT)) = 1;\n",
    "                \"\"\".format(cls.__db_name, \n",
    "                           to_table_name,\n",
    "                           cls.__db_name, \n",
    "                           from_table_name, \n",
    "                           string_atr, \n",
    "                           str(sample_rate))\n",
    "\n",
    "            res = execute_query(sql_query = ctas_sql_random_sampling)\n",
    "            return 1\n",
    "       \n",
    "        except Exception as message:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_n_days_as_mst_yyyymmdd(n):\n",
    "    lst=[]\n",
    "    for i in range(2,n):\n",
    "        date_i_days_ago = datetime.now() - timedelta(days=i)\n",
    "        lst.append(date_i_days_ago.strftime (\"%Y%m%d\")) \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_consp_member_id_summary_001(param_dict, util):\n",
    "    with open('consp_member_id_summary_t001.sql', 'r') as query:\n",
    "        ctas_query_copy_consp_member_id_summary_t001 = query.read().format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_copy_consp_member_id_summary_t001)\n",
    "    # While the table is partioned need to be repaired\n",
    "    ctas_query_repair_table_consp_member_id_summary_t001 = \"\"\"MSCK REPAIR TABLE {db_name}.consp_member_id_summary_t001;\"\"\".format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_repair_table_consp_member_id_summary_t001)\n",
    "    # While the table is partioned need to be repaired\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_consp_member_id_summary_007(param_dict, util):\n",
    "    with open('consp_member_id_summary_t007.sql', 'r') as query:\n",
    "        ctas_query_copy_consp_member_id_summary_t007 = query.read().format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_copy_consp_member_id_summary_t007)\n",
    "    # While the table is partioned need to be repaired\n",
    "    ctas_query_repair_table_consp_member_id_summary_t007 = \"\"\"MSCK REPAIR TABLE {db_name}.consp_member_id_summary_t007;\"\"\".format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_repair_table_consp_member_id_summary_t007)\n",
    "    # While the table is partioned need to be repaired\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_consp_member_id_summary_030(param_dict, util):\n",
    "    with open('consp_member_id_summary_t030.sql', 'r') as query:\n",
    "        ctas_query_copy_consp_member_id_summary_t030 = query.read().format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_copy_consp_member_id_summary_t030)\n",
    "    # While the table is partioned need to be repaired\n",
    "    ctas_query_repair_table_consp_member_id_summary_t030 = \"\"\"MSCK REPAIR TABLE {db_name}.consp_member_id_summary_t030;\"\"\".format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_repair_table_consp_member_id_summary_t030)\n",
    "    # While the table is partioned need to be repaired\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_consp_member_id_summary_060(param_dict, util):\n",
    "    with open('consp_member_id_summary_t060.sql', 'r') as query:\n",
    "        ctas_query_copy_consp_member_id_summary_t060 = query.read().format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_copy_consp_member_id_summary_t060)\n",
    "    # While the table is partioned need to be repaired\n",
    "    ctas_query_repair_table_consp_member_id_summary_t060 = \"\"\"MSCK REPAIR TABLE {db_name}.consp_member_id_summary_t060;\"\"\".format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_repair_table_consp_member_id_summary_t060)\n",
    "    # While the table is partioned need to be repaired\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_consp_member_id_summary_090(param_dict, util):\n",
    "    with open('consp_member_id_summary_t090.sql', 'r') as query:\n",
    "        ctas_query_copy_consp_member_id_summary_t090 = query.read().format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_copy_consp_member_id_summary_t090)\n",
    "    # While the table is partioned need to be repaired\n",
    "    ctas_query_repair_table_consp_member_id_summary_t090 = \"\"\"MSCK REPAIR TABLE {db_name}.consp_member_id_summary_t090;\"\"\".format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_repair_table_consp_member_id_summary_t090)\n",
    "    # While the table is partioned need to be repaired\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_over_qualtrics_data(param_dict, util):\n",
    "    with open('qualtrics_survey.sql', 'r') as query:\n",
    "        ctas_query_qualtrics_table_creation = query.read().format(**param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_query_qualtrics_table_creation)\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_dataset_001_007_030_060_090(param_dict, table_generation_date, sql_file_name, util):    \n",
    "    # Now Join Table consp_member_id_summary_t001, consp_member_id_summary_t007, consp_member_id_t030, consp_member_id_t060, consp_member_id_t090\n",
    "    # STEP 1 :\n",
    "    # Delete table if exists\n",
    "    param_dict['table_generation_date'] = table_generation_date\n",
    "    ctas_query_drop_table_db = \"\"\"DROP TABLE {db_name}.buyer_intent_training_set_001_007_030_060_090_{table_generation_date};\"\"\".format(**param_dict)\n",
    "    util.execute_query(sql_query = ctas_query_drop_table_db)\n",
    "\n",
    "    #STEP 2:\n",
    "    #Creat Data Set\n",
    "    with open(sql_file_name, 'r') as query:\n",
    "        ctas_training_set_001_007_030_060_090 = query.read().format(**param_dict)\n",
    "\n",
    "    res = util.execute_query(sql_query = ctas_training_set_001_007_030_060_090)\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return \"buyer_intent_training_set_001_007_030_060_090_{table_generation_date}\".format(**param_dict)\n",
    "    else:\n",
    "        return \"buyer_intent_training_set_001_007_030_060_090_{table_generation_date}\".format(**param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_two_tables(param_dict, table_1, table_2, util):\n",
    "    tmp_param_dict = param_dict.copy()\n",
    "    tmp_param_dict['generated_table'] = table_1[len(table_1)-2:] + table_2[len(table_1)-2:]\n",
    "    tmp_param_dict['table_1'] = table_1\n",
    "    tmp_param_dict['table_2'] = table_2\n",
    "    ctas_union_query = \"\"\"CREATE TABLE {db_name}.bi_tmp_union_{generated_table}\n",
    "    WITH (\n",
    "    external_location = '{s3_path_training_sets}/bi_tmp_union_{generated_table}',\n",
    "    format='PARQUET'\n",
    "    ) AS\n",
    "    SELECT * FROM {db_name}.{table_1}\n",
    "    UNION ALL\n",
    "    SELECT * FROM {db_name}.{table_2};\"\"\".format(**tmp_param_dict)\n",
    "    res = util.execute_query(sql_query = ctas_union_query)\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return \"bi_tmp_union_{generated_table}\".format(**tmp_param_dict)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_all_tables(param_dict, table_list, util):\n",
    "    final_table = functools.reduce((lambda table_1, table_2: union_two_tables(param_dict, table_1, table_2, util)), table_list)\n",
    "    return final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time_to_mst(time_str):\n",
    "    try:\n",
    "        date = datetime.strptime(time_str, '%m/%d/%Y %H:%M')\n",
    "        return date.strftime (\"%Y%m%d\")\n",
    "    except Exception as message:\n",
    "        return -1\n",
    "\n",
    "def transform_qualtrics_survey(input_file_name, output_file_name):\n",
    "    try:\n",
    "        df_qulatrics_survey = pd.read_csv(input_file_name)\n",
    "        df_qulatrics_survey = df_qulatrics_survey.iloc[2:]\n",
    "        df_qulatrics_survey = df_qulatrics_survey[['StartDate','EndDate', 'Member_id', 'Stagelabel1','Stagelabel2','Stagelabel3']]\n",
    "        df_qulatrics_survey = df_qulatrics_survey.rename(columns={'StartDate':'start_date','EndDate':'end_date', 'Member_id':'member_id', 'Stagelabel1':'stage_label_1','Stagelabel2':'stage_label_2','Stagelabel3':'stage_label_3'})\n",
    "        df_qulatrics_survey = df_qulatrics_survey[['member_id','start_date', 'end_date', 'stage_label_1', 'stage_label_2', 'stage_label_3']]\n",
    "        df_qulatrics_survey['start_date'] = df_qulatrics_survey['start_date'].apply(parse_time_to_mst)\n",
    "        df_qulatrics_survey['end_date'] = df_qulatrics_survey['end_date'].apply(parse_time_to_mst)\n",
    "        df_qulatrics_survey.to_csv(output_file_name, index = False)\n",
    "        return 1\n",
    "    except Exception as message:\n",
    "        return -1  \n",
    "\n",
    "#transform_qualtrics_survey('Stage_survey_data_2020.csv', 'qualtrics_buyer_stage_20200202.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_training_data(param_dict, data_set_table, qualtrics_table, sql_query_file, table_generation_date, util):\n",
    "    param_dict['table_generation_date'] = table_generation_date\n",
    "    param_dict['data_set_table'] = data_set_table\n",
    "    param_dict['qualtrics_survey'] = qualtrics_table\n",
    "    param_dict['table_generation_date'] = table_generation_date\n",
    "\n",
    "    with open(sql_query_file, 'r') as query:\n",
    "        ctas_label_data = query.read().format(**param_dict)\n",
    "\n",
    "    res = util.execute_query(sql_query = ctas_label_data)\n",
    "    if res['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        logger.info('STEP 1: Read Configuration File')\n",
    "        param_dict = read_config_file('ds_generation_config_params.json')\n",
    "        util = create_db(param_dict)\n",
    "\n",
    "        logger.info('STEP 2: Clean Up All tables generated in DB')\n",
    "        drop_tables_in_db(param_dict, util)\n",
    "        \n",
    "        logger.info('STEP 3: Clean Up all related s3 locations for tables')\n",
    "        cmd = 'aws s3 rm --recursive {s3_path_training_sets}'.format(**param_dict)\n",
    "        res = os.system(cmd)\n",
    "        if res == 0:\n",
    "            logger.info('STEP 3.1: Cleaning is done successfully')\n",
    "\n",
    "        logger.info('STEP 4: Copy CAP Summery for Member_Id for 001, 007, 030, 060, 090')\n",
    "        res_001 = copy_consp_member_id_summary_001(param_dict, util)\n",
    "        res_007 = copy_consp_member_id_summary_007(param_dict, util)\n",
    "        res_030 = copy_consp_member_id_summary_030(param_dict, util)\n",
    "        res_060 = copy_consp_member_id_summary_060(param_dict, util)\n",
    "        res_090 = copy_consp_member_id_summary_090(param_dict, util)\n",
    "\n",
    "        logger.info('STEP 5: Creat Table over Qualtrics Survey Data')\n",
    "        create_table_over_qualtrics_data(param_dict, util)\n",
    "        \n",
    "        # Generate Training Data Set for Specific Time got from Qualtrics\n",
    "        # Some users are answered couple of time while the servey was sending in diffrent batch from marketing and UX group\n",
    "        qualtrics_survey_answered_days = ['20200209','20200202','20200207','20200117','20200204','20200131','20200205','20200211','20200210','20200206','20200208','20200201','20200203']\n",
    "        lst_next_day = ['20200208','20200201','20200206','20200116','20200203','20200130','20200204','20200210','20200211','20200207','20200207','20200131','20200202']\n",
    "\n",
    "        for i in lst_next_day:\n",
    "            if i not in qualtrics_survey_answered_days:\n",
    "                qualtrics_survey_answered_days.append(i)\n",
    "\n",
    "        table_list = []\n",
    "\n",
    "        logger.info('STEP 6: Generate the required training dataset related to the answered time of users')\n",
    "        for elem in qualtrics_survey_answered_days:\n",
    "            table_list.append(make_training_dataset_001_007_030_060_090(param_dict, elem, \n",
    "                                                                        'buyer_intent_training_set_001_007_030_060_090_outlier_filter.sql', util))\n",
    "        \n",
    "        logger.info('STEP 7: We need to union of all data sets while we want to join with the users answer. We should make sure that we have all users behavioural data for labeling job')\n",
    "        final_union_table = union_all_tables(param_dict, table_list, util)\n",
    "        logger.info(\"The final unioned table is :%s\", final_union_table )\n",
    "        \n",
    "        logger.info('STEP 8 : label useres behabiour beased on their answers')\n",
    "        res = label_training_data(param_dict, final_union_table, 'qualtrics_survey','buyer_intent_label_data_set.sql' ,'final', util )\n",
    "        return 1\n",
    "\n",
    "    except Exception as message:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        f_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        logger.error(\"filename = %s - function = main - exception in line_number = %s - hint = %s \" % (\n",
    "                f_name, exc_tb.tb_lineno, message.__str__()))\n",
    "        return -1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-13 22:28:35.260 INFO <ipython-input-21-ded20d39fcf5>.3 - STEP 1: Read Configuration File\n",
      "2020-02-13 22:28:35.261 INFO athena_util.py.111 - Read config from default schema\n",
      "2020-02-13 22:28:35.262 INFO athena_util.py.114 - Schema fetched from default config file:\n",
      "2020-02-13 22:28:35.262 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:35.381 INFO athena_util.py.462 - 2020-02-13 22:28:35.381728\n",
      "2020-02-13 22:28:36.416 INFO athena_util.py.464 - 2020-02-13 22:28:36.416537\n",
      "2020-02-13 22:28:36.777 INFO <ipython-input-21-ded20d39fcf5>.7 - STEP 2: Clean Up All tables generated in DB\n",
      "2020-02-13 22:28:36.777 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:36.858 INFO athena_util.py.462 - 2020-02-13 22:28:36.858838\n",
      "2020-02-13 22:28:37.897 INFO athena_util.py.464 - 2020-02-13 22:28:37.897828\n",
      "2020-02-13 22:28:37.999 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:38.115 INFO athena_util.py.462 - 2020-02-13 22:28:38.115571\n",
      "2020-02-13 22:28:39.662 INFO athena_util.py.464 - 2020-02-13 22:28:39.662455\n",
      "2020-02-13 22:28:39.735 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:39.811 INFO athena_util.py.462 - 2020-02-13 22:28:39.811294\n",
      "2020-02-13 22:28:41.364 INFO athena_util.py.464 - 2020-02-13 22:28:41.364934\n",
      "2020-02-13 22:28:41.420 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:41.490 INFO athena_util.py.462 - 2020-02-13 22:28:41.490347\n",
      "2020-02-13 22:28:43.042 INFO athena_util.py.464 - 2020-02-13 22:28:43.042614\n",
      "2020-02-13 22:28:43.103 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:43.274 INFO athena_util.py.462 - 2020-02-13 22:28:43.274127\n",
      "2020-02-13 22:28:44.821 INFO athena_util.py.464 - 2020-02-13 22:28:44.821302\n",
      "2020-02-13 22:28:44.888 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:44.951 INFO athena_util.py.462 - 2020-02-13 22:28:44.951775\n",
      "2020-02-13 22:28:46.494 INFO athena_util.py.464 - 2020-02-13 22:28:46.494147\n",
      "2020-02-13 22:28:46.558 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:46.631 INFO athena_util.py.462 - 2020-02-13 22:28:46.631090\n",
      "2020-02-13 22:28:48.186 INFO athena_util.py.464 - 2020-02-13 22:28:48.186737\n",
      "2020-02-13 22:28:48.254 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:48.325 INFO athena_util.py.462 - 2020-02-13 22:28:48.325011\n",
      "2020-02-13 22:28:49.866 INFO athena_util.py.464 - 2020-02-13 22:28:49.866932\n",
      "2020-02-13 22:28:49.944 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:50.023 INFO athena_util.py.462 - 2020-02-13 22:28:50.023160\n",
      "2020-02-13 22:28:51.564 INFO athena_util.py.464 - 2020-02-13 22:28:51.564665\n",
      "2020-02-13 22:28:51.623 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:51.689 INFO athena_util.py.462 - 2020-02-13 22:28:51.689416\n",
      "2020-02-13 22:28:53.271 INFO athena_util.py.464 - 2020-02-13 22:28:53.271902\n",
      "2020-02-13 22:28:53.325 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:53.463 INFO athena_util.py.462 - 2020-02-13 22:28:53.463505\n",
      "2020-02-13 22:28:55.013 INFO athena_util.py.464 - 2020-02-13 22:28:55.013011\n",
      "2020-02-13 22:28:55.057 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:55.141 INFO athena_util.py.462 - 2020-02-13 22:28:55.141338\n",
      "2020-02-13 22:28:56.692 INFO athena_util.py.464 - 2020-02-13 22:28:56.692204\n",
      "2020-02-13 22:28:56.751 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:56.881 INFO athena_util.py.462 - 2020-02-13 22:28:56.881507\n",
      "2020-02-13 22:28:58.432 INFO athena_util.py.464 - 2020-02-13 22:28:58.431970\n",
      "2020-02-13 22:28:58.475 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:28:58.558 INFO athena_util.py.462 - 2020-02-13 22:28:58.558485\n",
      "2020-02-13 22:29:00.098 INFO athena_util.py.464 - 2020-02-13 22:29:00.098799\n",
      "2020-02-13 22:29:00.174 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:00.250 INFO athena_util.py.462 - 2020-02-13 22:29:00.250582\n",
      "2020-02-13 22:29:01.793 INFO athena_util.py.464 - 2020-02-13 22:29:01.793211\n",
      "2020-02-13 22:29:01.871 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:01.994 INFO athena_util.py.462 - 2020-02-13 22:29:01.994613\n",
      "2020-02-13 22:29:03.541 INFO athena_util.py.464 - 2020-02-13 22:29:03.541843\n",
      "2020-02-13 22:29:03.590 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:03.659 INFO athena_util.py.462 - 2020-02-13 22:29:03.659628\n",
      "2020-02-13 22:29:05.203 INFO athena_util.py.464 - 2020-02-13 22:29:05.203779\n",
      "2020-02-13 22:29:05.262 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:05.338 INFO athena_util.py.462 - 2020-02-13 22:29:05.338476\n",
      "2020-02-13 22:29:06.880 INFO athena_util.py.464 - 2020-02-13 22:29:06.880615\n",
      "2020-02-13 22:29:06.951 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:07.023 INFO athena_util.py.462 - 2020-02-13 22:29:07.023758\n",
      "2020-02-13 22:29:08.573 INFO athena_util.py.464 - 2020-02-13 22:29:08.573335\n",
      "2020-02-13 22:29:08.648 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:08.723 INFO athena_util.py.462 - 2020-02-13 22:29:08.723536\n",
      "2020-02-13 22:29:10.271 INFO athena_util.py.464 - 2020-02-13 22:29:10.271345\n",
      "2020-02-13 22:29:10.329 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:10.544 INFO athena_util.py.462 - 2020-02-13 22:29:10.544007\n",
      "2020-02-13 22:29:12.091 INFO athena_util.py.464 - 2020-02-13 22:29:12.091890\n",
      "2020-02-13 22:29:12.191 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:12.262 INFO athena_util.py.462 - 2020-02-13 22:29:12.262386\n",
      "2020-02-13 22:29:13.805 INFO athena_util.py.464 - 2020-02-13 22:29:13.805059\n",
      "2020-02-13 22:29:13.851 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:13.982 INFO athena_util.py.462 - 2020-02-13 22:29:13.982478\n",
      "2020-02-13 22:29:15.523 INFO athena_util.py.464 - 2020-02-13 22:29:15.523795\n",
      "2020-02-13 22:29:15.578 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:15.656 INFO athena_util.py.462 - 2020-02-13 22:29:15.656766\n",
      "2020-02-13 22:29:17.196 INFO athena_util.py.464 - 2020-02-13 22:29:17.196354\n",
      "2020-02-13 22:29:17.261 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:17.569 INFO athena_util.py.462 - 2020-02-13 22:29:17.569495\n",
      "2020-02-13 22:29:19.110 INFO athena_util.py.464 - 2020-02-13 22:29:19.109999\n",
      "2020-02-13 22:29:19.156 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:19.295 INFO athena_util.py.462 - 2020-02-13 22:29:19.295750\n",
      "2020-02-13 22:29:20.837 INFO athena_util.py.464 - 2020-02-13 22:29:20.837230\n",
      "2020-02-13 22:29:20.892 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:20.967 INFO athena_util.py.462 - 2020-02-13 22:29:20.967666\n",
      "2020-02-13 22:29:22.520 INFO athena_util.py.464 - 2020-02-13 22:29:22.520794\n",
      "2020-02-13 22:29:22.584 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:22.664 INFO athena_util.py.462 - 2020-02-13 22:29:22.664134\n",
      "2020-02-13 22:29:24.204 INFO athena_util.py.464 - 2020-02-13 22:29:24.204446\n",
      "2020-02-13 22:29:24.257 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:24.327 INFO athena_util.py.462 - 2020-02-13 22:29:24.327520\n",
      "2020-02-13 22:29:25.870 INFO athena_util.py.464 - 2020-02-13 22:29:25.870892\n",
      "2020-02-13 22:29:25.922 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:25.994 INFO athena_util.py.462 - 2020-02-13 22:29:25.994960\n",
      "2020-02-13 22:29:27.536 INFO athena_util.py.464 - 2020-02-13 22:29:27.536640\n",
      "2020-02-13 22:29:27.585 INFO <ipython-input-21-ded20d39fcf5>.10 - STEP 3: Clean Up all related s3 locations for tables\n",
      "2020-02-13 22:29:28.122 INFO <ipython-input-21-ded20d39fcf5>.14 - STEP 3.1: Cleaning is done successfully\n",
      "2020-02-13 22:29:28.123 INFO <ipython-input-21-ded20d39fcf5>.16 - STEP 4: Copy CAP Summery for Member_Id for 001, 007, 030, 060, 090\n",
      "2020-02-13 22:29:28.124 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:28.220 INFO athena_util.py.462 - 2020-02-13 22:29:28.220630\n",
      "2020-02-13 22:29:29.258 INFO athena_util.py.464 - 2020-02-13 22:29:29.258274\n",
      "2020-02-13 22:29:29.310 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:29:29.391 INFO athena_util.py.462 - 2020-02-13 22:29:29.391005\n",
      "2020-02-13 22:30:05.608 INFO athena_util.py.464 - 2020-02-13 22:30:05.608531\n",
      "2020-02-13 22:30:05.716 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:30:05.790 INFO athena_util.py.462 - 2020-02-13 22:30:05.790950\n",
      "2020-02-13 22:30:06.820 INFO athena_util.py.464 - 2020-02-13 22:30:06.820865\n",
      "2020-02-13 22:30:06.888 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:30:07.020 INFO athena_util.py.462 - 2020-02-13 22:30:07.020040\n",
      "2020-02-13 22:30:36.824 INFO athena_util.py.464 - 2020-02-13 22:30:36.824755\n",
      "2020-02-13 22:30:36.979 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:30:37.087 INFO athena_util.py.462 - 2020-02-13 22:30:37.087896\n",
      "2020-02-13 22:30:38.634 INFO athena_util.py.464 - 2020-02-13 22:30:38.634015\n",
      "2020-02-13 22:30:38.687 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:30:38.851 INFO athena_util.py.462 - 2020-02-13 22:30:38.851633\n",
      "2020-02-13 22:31:05.223 INFO athena_util.py.464 - 2020-02-13 22:31:05.223431\n",
      "2020-02-13 22:31:05.361 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:31:05.479 INFO athena_util.py.462 - 2020-02-13 22:31:05.479973\n",
      "2020-02-13 22:31:06.508 INFO athena_util.py.464 - 2020-02-13 22:31:06.508295\n",
      "2020-02-13 22:31:06.571 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:31:06.653 INFO athena_util.py.462 - 2020-02-13 22:31:06.653905\n",
      "2020-02-13 22:31:37.552 INFO athena_util.py.464 - 2020-02-13 22:31:37.552111\n",
      "2020-02-13 22:31:37.658 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:31:37.782 INFO athena_util.py.462 - 2020-02-13 22:31:37.782768\n",
      "2020-02-13 22:31:38.811 INFO athena_util.py.464 - 2020-02-13 22:31:38.811581\n",
      "2020-02-13 22:31:38.872 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:31:39.127 INFO athena_util.py.462 - 2020-02-13 22:31:39.127747\n",
      "2020-02-13 22:31:54.033 INFO athena_util.py.464 - 2020-02-13 22:31:54.033426\n",
      "2020-02-13 22:31:54.135 INFO <ipython-input-21-ded20d39fcf5>.23 - STEP 5: Creat Table over Qualtrics Survey Data\n",
      "2020-02-13 22:31:54.137 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:31:54.210 INFO athena_util.py.462 - 2020-02-13 22:31:54.210542\n",
      "2020-02-13 22:31:55.241 INFO athena_util.py.464 - 2020-02-13 22:31:55.241586\n",
      "2020-02-13 22:31:55.318 INFO <ipython-input-21-ded20d39fcf5>.37 - STEP 6: Generate the required training dataset related to the answered time of users\n",
      "2020-02-13 22:31:55.319 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:31:55.476 INFO athena_util.py.462 - 2020-02-13 22:31:55.476883\n",
      "2020-02-13 22:31:57.020 INFO athena_util.py.464 - 2020-02-13 22:31:57.020518\n",
      "2020-02-13 22:31:57.096 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:31:57.356 INFO athena_util.py.462 - 2020-02-13 22:31:57.356610\n",
      "2020-02-13 22:32:22.089 INFO athena_util.py.464 - 2020-02-13 22:32:22.089534\n",
      "2020-02-13 22:32:22.144 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:32:22.220 INFO athena_util.py.462 - 2020-02-13 22:32:22.220348\n",
      "2020-02-13 22:32:23.763 INFO athena_util.py.464 - 2020-02-13 22:32:23.763349\n",
      "2020-02-13 22:32:23.841 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:32:24.087 INFO athena_util.py.462 - 2020-02-13 22:32:24.087815\n",
      "2020-02-13 22:32:48.990 INFO athena_util.py.464 - 2020-02-13 22:32:48.990379\n",
      "2020-02-13 22:32:49.064 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:32:49.131 INFO athena_util.py.462 - 2020-02-13 22:32:49.131241\n",
      "2020-02-13 22:32:51.184 INFO athena_util.py.464 - 2020-02-13 22:32:51.184532\n",
      "2020-02-13 22:32:51.261 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:32:51.445 INFO athena_util.py.462 - 2020-02-13 22:32:51.445742\n",
      "2020-02-13 22:33:11.807 INFO athena_util.py.464 - 2020-02-13 22:33:11.807329\n",
      "2020-02-13 22:33:11.866 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:33:12.020 INFO athena_util.py.462 - 2020-02-13 22:33:12.020318\n",
      "2020-02-13 22:33:14.076 INFO athena_util.py.464 - 2020-02-13 22:33:14.076494\n",
      "2020-02-13 22:33:14.166 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:33:14.352 INFO athena_util.py.462 - 2020-02-13 22:33:14.352704\n",
      "2020-02-13 22:33:33.352 INFO athena_util.py.464 - 2020-02-13 22:33:33.352284\n",
      "2020-02-13 22:33:33.421 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:33:33.761 INFO athena_util.py.462 - 2020-02-13 22:33:33.761262\n",
      "2020-02-13 22:33:35.306 INFO athena_util.py.464 - 2020-02-13 22:33:35.306915\n",
      "2020-02-13 22:33:35.422 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:33:35.828 INFO athena_util.py.462 - 2020-02-13 22:33:35.828442\n",
      "2020-02-13 22:33:58.118 INFO athena_util.py.464 - 2020-02-13 22:33:58.118766\n",
      "2020-02-13 22:33:58.188 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:33:58.259 INFO athena_util.py.462 - 2020-02-13 22:33:58.259533\n",
      "2020-02-13 22:34:00.322 INFO athena_util.py.464 - 2020-02-13 22:34:00.322892\n",
      "2020-02-13 22:34:00.414 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:34:00.602 INFO athena_util.py.462 - 2020-02-13 22:34:00.602076\n",
      "2020-02-13 22:34:19.712 INFO athena_util.py.464 - 2020-02-13 22:34:19.712668\n",
      "2020-02-13 22:34:19.771 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:34:19.854 INFO athena_util.py.462 - 2020-02-13 22:34:19.854007\n",
      "2020-02-13 22:34:21.915 INFO athena_util.py.464 - 2020-02-13 22:34:21.915377\n",
      "2020-02-13 22:34:22.005 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:34:22.205 INFO athena_util.py.462 - 2020-02-13 22:34:22.205480\n",
      "2020-02-13 22:34:46.076 INFO athena_util.py.464 - 2020-02-13 22:34:46.076795\n",
      "2020-02-13 22:34:46.131 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:34:46.232 INFO athena_util.py.462 - 2020-02-13 22:34:46.232120\n",
      "2020-02-13 22:34:47.773 INFO athena_util.py.464 - 2020-02-13 22:34:47.773866\n",
      "2020-02-13 22:34:47.862 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:34:48.086 INFO athena_util.py.462 - 2020-02-13 22:34:48.086469\n",
      "2020-02-13 22:35:08.098 INFO athena_util.py.464 - 2020-02-13 22:35:08.098462\n",
      "2020-02-13 22:35:08.162 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:35:08.241 INFO athena_util.py.462 - 2020-02-13 22:35:08.241457\n",
      "2020-02-13 22:35:10.318 INFO athena_util.py.464 - 2020-02-13 22:35:10.318261\n",
      "2020-02-13 22:35:10.426 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:35:10.700 INFO athena_util.py.462 - 2020-02-13 22:35:10.700113\n",
      "2020-02-13 22:35:31.379 INFO athena_util.py.464 - 2020-02-13 22:35:31.379868\n",
      "2020-02-13 22:35:31.434 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:35:31.504 INFO athena_util.py.462 - 2020-02-13 22:35:31.504367\n",
      "2020-02-13 22:35:33.573 INFO athena_util.py.464 - 2020-02-13 22:35:33.573526\n",
      "2020-02-13 22:35:34.188 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:35:34.368 INFO athena_util.py.462 - 2020-02-13 22:35:34.368914\n",
      "2020-02-13 22:36:06.056 INFO athena_util.py.464 - 2020-02-13 22:36:06.055972\n",
      "2020-02-13 22:36:06.115 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:36:06.200 INFO athena_util.py.462 - 2020-02-13 22:36:06.200327\n",
      "2020-02-13 22:36:08.253 INFO athena_util.py.464 - 2020-02-13 22:36:08.253301\n",
      "2020-02-13 22:36:08.316 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:36:08.478 INFO athena_util.py.462 - 2020-02-13 22:36:08.478689\n",
      "2020-02-13 22:36:31.358 INFO athena_util.py.464 - 2020-02-13 22:36:31.358261\n",
      "2020-02-13 22:36:31.435 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:36:31.510 INFO athena_util.py.462 - 2020-02-13 22:36:31.510871\n",
      "2020-02-13 22:36:33.568 INFO athena_util.py.464 - 2020-02-13 22:36:33.568665\n",
      "2020-02-13 22:36:33.705 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:36:33.884 INFO athena_util.py.462 - 2020-02-13 22:36:33.884497\n",
      "2020-02-13 22:36:53.835 INFO athena_util.py.464 - 2020-02-13 22:36:53.835610\n",
      "2020-02-13 22:36:53.897 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:36:53.973 INFO athena_util.py.462 - 2020-02-13 22:36:53.973871\n",
      "2020-02-13 22:36:56.039 INFO athena_util.py.464 - 2020-02-13 22:36:56.039076\n",
      "2020-02-13 22:36:56.146 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:36:56.425 INFO athena_util.py.462 - 2020-02-13 22:36:56.425269\n",
      "2020-02-13 22:37:16.401 INFO athena_util.py.464 - 2020-02-13 22:37:16.401796\n",
      "2020-02-13 22:37:16.447 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:37:16.524 INFO athena_util.py.462 - 2020-02-13 22:37:16.524273\n",
      "2020-02-13 22:37:18.579 INFO athena_util.py.464 - 2020-02-13 22:37:18.579460\n",
      "2020-02-13 22:37:18.689 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:37:19.028 INFO athena_util.py.462 - 2020-02-13 22:37:19.028301\n",
      "2020-02-13 22:37:39.673 INFO athena_util.py.464 - 2020-02-13 22:37:39.673263\n",
      "2020-02-13 22:37:39.729 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:37:39.802 INFO athena_util.py.462 - 2020-02-13 22:37:39.802062\n",
      "2020-02-13 22:37:41.903 INFO athena_util.py.464 - 2020-02-13 22:37:41.903130\n",
      "2020-02-13 22:37:42.004 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:37:42.212 INFO athena_util.py.462 - 2020-02-13 22:37:42.212223\n",
      "2020-02-13 22:38:02.456 INFO athena_util.py.464 - 2020-02-13 22:38:02.456552\n",
      "2020-02-13 22:38:02.513 INFO <ipython-input-21-ded20d39fcf5>.42 - STEP 7: We need to union of all data sets while we want to join with the users answer. We should make sure that we have all users behavioural data for labeling job\n",
      "2020-02-13 22:38:02.514 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:38:02.608 INFO athena_util.py.462 - 2020-02-13 22:38:02.608642\n",
      "2020-02-13 22:38:16.490 INFO athena_util.py.464 - 2020-02-13 22:38:16.490584\n",
      "2020-02-13 22:38:16.542 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:38:16.614 INFO athena_util.py.462 - 2020-02-13 22:38:16.614065\n",
      "2020-02-13 22:38:39.340 INFO athena_util.py.464 - 2020-02-13 22:38:39.340377\n",
      "2020-02-13 22:38:39.386 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:38:39.462 INFO athena_util.py.462 - 2020-02-13 22:38:39.462156\n",
      "2020-02-13 22:39:25.311 INFO athena_util.py.464 - 2020-02-13 22:39:25.311719\n",
      "2020-02-13 22:39:25.372 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:39:25.444 INFO athena_util.py.462 - 2020-02-13 22:39:25.444781\n",
      "2020-02-13 22:40:06.612 INFO athena_util.py.464 - 2020-02-13 22:40:06.612203\n",
      "2020-02-13 22:40:06.676 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:40:06.755 INFO athena_util.py.462 - 2020-02-13 22:40:06.755500\n",
      "2020-02-13 22:40:59.885 INFO athena_util.py.464 - 2020-02-13 22:40:59.885461\n",
      "2020-02-13 22:40:59.962 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:41:00.032 INFO athena_util.py.462 - 2020-02-13 22:41:00.032017\n",
      "2020-02-13 22:42:00.581 INFO athena_util.py.464 - 2020-02-13 22:42:00.581766\n",
      "2020-02-13 22:42:00.637 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:42:00.706 INFO athena_util.py.462 - 2020-02-13 22:42:00.706598\n",
      "2020-02-13 22:43:23.622 INFO athena_util.py.464 - 2020-02-13 22:43:23.622824\n",
      "2020-02-13 22:43:23.680 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:43:23.759 INFO athena_util.py.462 - 2020-02-13 22:43:23.759688\n",
      "2020-02-13 22:44:47.254 INFO athena_util.py.464 - 2020-02-13 22:44:47.254133\n",
      "2020-02-13 22:44:47.310 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:44:47.377 INFO athena_util.py.462 - 2020-02-13 22:44:47.377366\n",
      "2020-02-13 22:46:18.439 INFO athena_util.py.464 - 2020-02-13 22:46:18.439487\n",
      "2020-02-13 22:46:18.487 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:46:18.598 INFO athena_util.py.462 - 2020-02-13 22:46:18.598308\n",
      "2020-02-13 22:47:58.950 INFO athena_util.py.464 - 2020-02-13 22:47:58.950743\n",
      "2020-02-13 22:47:59.029 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:47:59.176 INFO athena_util.py.462 - 2020-02-13 22:47:59.176932\n",
      "2020-02-13 22:49:49.414 INFO athena_util.py.464 - 2020-02-13 22:49:49.414701\n",
      "2020-02-13 22:49:49.482 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:49:49.564 INFO athena_util.py.462 - 2020-02-13 22:49:49.564243\n",
      "2020-02-13 22:51:51.367 INFO athena_util.py.464 - 2020-02-13 22:51:51.367242\n",
      "2020-02-13 22:51:51.440 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:51:51.511 INFO athena_util.py.462 - 2020-02-13 22:51:51.511225\n",
      "2020-02-13 22:54:01.433 INFO athena_util.py.464 - 2020-02-13 22:54:01.433732\n",
      "2020-02-13 22:54:01.506 INFO athena_util.py.441 - s3://datascience-workspace-dev/buyer-intent/buyer_intent_data_sets\n",
      "2020-02-13 22:54:01.580 INFO athena_util.py.462 - 2020-02-13 22:54:01.580414\n",
      "2020-02-13 22:56:23.580 INFO athena_util.py.464 - 2020-02-13 22:56:23.580928\n",
      "2020-02-13 22:56:23.637 INFO <ipython-input-21-ded20d39fcf5>.44 - The final unioned table is :bi_tmp_union_16aining_set_001_007_030_060_090_20200130\n",
      "2020-02-13 22:56:23.638 INFO <ipython-input-21-ded20d39fcf5>.46 - STEP 8 : label useres behabiour beased on their answers\n",
      "2020-02-13 22:56:23.639 ERROR <ipython-input-21-ded20d39fcf5>.54 - filename = <ipython-input-21-ded20d39fcf5> - function = main - exception in line_number = 47 - hint = [Errno 2] No such file or directory: 'buyer_intent_label_data_set.sql' \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
